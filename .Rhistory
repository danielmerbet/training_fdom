#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
#data$yday <- yday(data$date)
data$cyday <- cos(yday(data$date)/365)
#data$syday <- sin(yday(data$date)/365)
data$random <- runif(nrow(data))
plot(data$swt, data$fdom)
plot(data$swt, data$fdom)
library(lubridate); library(dplyr)
#soil and meteo data from Open Meteo
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
soil_meteo <- read.csv(paste0(dir, "download/meteo-soil_data.csv"))
soil_meteo$date <- as.Date(soil_meteo$date)
soil_meteo_temp <- aggregate(. ~ date, data=soil_meteo, FUN = function(x) c(mean = mean(x, na.rm = TRUE)))
soil_meteo_temp$precipitation <- NULL
rain_temp <- aggregate(precipitation ~ date, data=soil_meteo, FUN = function(x) c(sum = sum(x, na.rm = TRUE)))
soil_meteo <- merge(soil_meteo_temp, rain_temp, by="date")
names(soil_meteo) <- c("date","t", "rh", "sp", "cc", "pev",
"ws","st7", "st28", "st100","st255",
"sm7", "sm28", "sm100","sm255","tp")
soil_meteo_temp <- NULL
rain_temp <- NULL
write.csv(soil_meteo, paste0(dir, "data/meteo-soil_daily_data.csv"),
row.names = F, quote = F)
#streamflow data from ATL
#stream <- read.csv(paste0(dir, "data/river.csv"), header = F)
#stream <- data.frame(date=as.Date(stream$V1, "%m/%d/%Y"), q=stream$V2)
#Load lake data (GLM output)
glm_out <- read.csv(paste0(dir, "data/lake.csv"))
lake_data <- data.frame(date=as.Date(glm_out$time),
swt=glm_out$Surface.Temp,
v=glm_out$Volume,
lh=glm_out$Daily.Qe,
sh=glm_out$Daily.Qh,
light=glm_out$Light,
strat=glm_out$Max.dT.dz)
#merge all possible drivers
#drivers <- merge(stream, lake_data, by="date")
#drivers <- merge(drivers, soil_meteo, by="date")
drivers <- merge(lake_data, soil_meteo, by="date")
#load GWLF: discharge and DOC
GWLF <- read.csv(paste0(dir, "data/GWLF.csv"), header = T)
GWLF <- GWLF[,2:ncol(GWLF)]
colnames(GWLF) <- c("date", "q_gwlf", "doc_gwlf")
GWLF$date <- as.Date(GWLF$date, "%m/%d/%Y")
drivers <- merge(drivers, GWLF, by="date")
#load DOC
DOC <- read.csv("sau/data/DOC_SAU_C1.csv")
View(DOC)
DOC <- data.frame(date=DOC$Fecha, depth=DOC$Depth, doc=DOC$Valor)
View(DOC)
?subset
subset(DOC, depth==5)
DOC <- subset(DOC, depth==5)
View(DOC)
drivers <- merge(drivers, DOC, by="date")
write.csv(drivers, paste0(dir,"data/drivers.csv"), row.names = F, quote = F)
#load DOC
DOC <- read.csv("sau/data/DOC_SAU_C1.csv")
DOC <- data.frame(date=DOC$Fecha, depth=DOC$Depth, doc=DOC$Valor)
drivers <- merge(drivers, GWLF, by="date")
#merge all possible drivers
#drivers <- merge(stream, lake_data, by="date")
#drivers <- merge(drivers, soil_meteo, by="date")
drivers <- merge(lake_data, soil_meteo, by="date")
#load GWLF: discharge and DOC
GWLF <- read.csv(paste0(dir, "data/GWLF.csv"), header = T)
GWLF <- GWLF[,2:ncol(GWLF)]
colnames(GWLF) <- c("date", "q_gwlf", "doc_gwlf")
GWLF$date <- as.Date(GWLF$date, "%m/%d/%Y")
drivers <- merge(drivers, GWLF, by="date")
#load DOC
DOC <- read.csv("sau/data/DOC_SAU_C1.csv")
DOC <- data.frame(date=DOC$Fecha, depth=DOC$Depth, doc=DOC$Valor)
DOC <- subset(DOC, depth==5)
View(drivers)
DOC$date <- as.Date(DOC$date)
drivers <- merge(drivers, DOC, by="date")
#load DOC
DOC <- read.csv("sau/data/DOC_SAU_C1.csv")
DOC <- data.frame(date=DOC$Fecha, depth=DOC$Depth, doc=DOC$Valor)
DOC <- subset(DOC, depth==5)
#load DOC
DOC <- read.csv("sau/data/DOC_SAU_C1.csv")
DOC <- data.frame(date=DOC$Fecha, depth=DOC$Depth, doc=DOC$Valor)
DOC
#load DOC
DOC <- read.csv("sau/data/DOC_SAU_C1.csv")
DOC <- data.frame(date=DOC$Fecha, depth=DOC$Depth, doc=DOC$Valor)
#DOC <- subset(DOC, depth==5)
DOC <- subset(data, depth >= 0 & depth <= 5)
DOC$date <- as.Date(DOC$date)
#Load lake data (GLM output)
glm_out <- read.csv(paste0(dir, "data/lake.csv"))
lake_data <- data.frame(date=as.Date(glm_out$time),
swt=glm_out$Surface.Temp,
v=glm_out$Volume,
lh=glm_out$Daily.Qe,
sh=glm_out$Daily.Qh,
light=glm_out$Light,
strat=glm_out$Max.dT.dz)
#merge all possible drivers
#drivers <- merge(stream, lake_data, by="date")
#drivers <- merge(drivers, soil_meteo, by="date")
drivers <- merge(lake_data, soil_meteo, by="date")
#load GWLF: discharge and DOC
GWLF <- read.csv(paste0(dir, "data/GWLF.csv"), header = T)
GWLF <- GWLF[,2:ncol(GWLF)]
colnames(GWLF) <- c("date", "q_gwlf", "doc_gwlf")
GWLF$date <- as.Date(GWLF$date, "%m/%d/%Y")
drivers <- merge(drivers, GWLF, by="date")
#load DOC
DOC <- read.csv("sau/data/DOC_SAU_C1.csv")
DOC <- data.frame(date=DOC$Fecha, depth=DOC$Depth, doc=DOC$Valor)
#DOC <- subset(DOC, depth==5)
DOC <- subset(data, depth >= 0 & depth <= 5)
#DOC <- subset(DOC, depth==5)
DOC <- subset(DOC, depth >= 0 & depth <= 5)
DOC$date <- as.Date(DOC$date)
drivers <- merge(drivers, DOC, by="date")
write.csv(drivers, paste0(dir,"data/drivers.csv"), row.names = F, quote = F)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#seasonal forecast has only soil temperature and soil moisture for first level
row_names_to_remove <- c("st28", "st100", "st255", "sm28", "sm100", "sm255")
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
#data$yday <- yday(data$date)
data$cyday <- cos(yday(data$date)/365)
#data$syday <- sin(yday(data$date)/365)
data$random <- runif(nrow(data))
#ML Analysis
###############################################################
#Random forest
library(randomForest)
save_rsq <- c()
save_rmse <- c()
#1. TRAINING WITH ALL DATA AND TESTING WITH SAMPLES OOB
#in theory this is wrong because the trianing data is autocorrelated
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 1000)
#testing: check with data not used in training but still autocorrelated (Out-of-Bag data)
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, data[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((data[tvar][,1] - predRF)^2)), 2); rmseOOB
importance_random <- importance(RFfit); importance_random
importance_random/sum(importance_random)*100
#stats are quite good, it seems promising
plot(data$date, data[tvar][,1], xlab="Dates", ylab="fDOM (QSU)")
points(data$date, predRF, col="darkgrey")
plot(data[tvar][,1],predRF, xlab="Obs", ylab="Sim", ylim=c(5,52),xlim=c(5,52))
abline(0,1, col="red")
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.8 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
View(data)
plot(data$doc, data$fdom)
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "doc"#"fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom"#"fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
View(data)
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
View(drivers)
data <- drivers
View(data)
#merge all and add julian day and dummy
#data <- merge(drivers, target, by="date")
#data$yday <- yday(data$date)
data$cyday <- cos(yday(data$date)/365)
#data$syday <- sin(yday(data$date)/365)
data$random <- runif(nrow(data))
#ML Analysis
###############################################################
#Random forest
library(randomForest)
save_rsq <- c()
save_rmse <- c()
#1. TRAINING WITH ALL DATA AND TESTING WITH SAMPLES OOB
#in theory this is wrong because the trianing data is autocorrelated
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 1000)
#load target variables
tvar <- "doc"#"fdom"
#1. TRAINING WITH ALL DATA AND TESTING WITH SAMPLES OOB
#in theory this is wrong because the trianing data is autocorrelated
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 1000)
#testing: check with data not used in training but still autocorrelated (Out-of-Bag data)
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, data[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((data[tvar][,1] - predRF)^2)), 2); rmseOOB
importance_random <- importance(RFfit); importance_random
importance_random/sum(importance_random)*100
#stats are quite good, it seems promising
plot(data$date, data[tvar][,1], xlab="Dates", ylab="fDOM (QSU)")
points(data$date, predRF, col="darkgrey")
plot(data[tvar][,1],predRF, xlab="Obs", ylab="Sim", ylim=c(5,52),xlim=c(5,52))
abline(0,1, col="red")
plot(data[tvar][,1],predRF, xlab="Obs", ylab="Sim") #ylim=c(5,52),xlim=c(5,52)
abline(0,1, col="red")
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.8 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
abline(v = as.numeric(as.Date(testdata$date[1])), col="darkgrey", lwd=2, lty=2)
importance_random/sum(importance_random)*100
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom" #"doc"#"fdom"
#load target variables
tvar <- "fdom" #"doc"#"fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
data <- drivers
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
#data$yday <- yday(data$date)
data$cyday <- cos(yday(data$date)*pi/180)
#data$syday <- sin(yday(data$date)/365)
data$random <- runif(nrow(data))
plot(data$cyday)
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
library(lubridate); library(dplyr)
#soil and meteo data from Open Meteo
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
soil_meteo <- read.csv(paste0(dir, "download/meteo-soil_data.csv"))
soil_meteo$date <- as.Date(soil_meteo$date)
soil_meteo_temp <- aggregate(. ~ date, data=soil_meteo, FUN = function(x) c(mean = mean(x, na.rm = TRUE)))
soil_meteo_temp$precipitation <- NULL
rain_temp <- aggregate(precipitation ~ date, data=soil_meteo, FUN = function(x) c(sum = sum(x, na.rm = TRUE)))
soil_meteo <- merge(soil_meteo_temp, rain_temp, by="date")
names(soil_meteo) <- c("date","t", "rh", "sp", "cc", "pev",
"ws","st7", "st28", "st100","st255",
"sm7", "sm28", "sm100","sm255","tp")
soil_meteo_temp <- NULL
rain_temp <- NULL
write.csv(soil_meteo, paste0(dir, "data/meteo-soil_daily_data.csv"),
row.names = F, quote = F)
#streamflow data from ATL
#stream <- read.csv(paste0(dir, "data/river.csv"), header = F)
#stream <- data.frame(date=as.Date(stream$V1, "%m/%d/%Y"), q=stream$V2)
#Load lake data (GLM output)
glm_out <- read.csv(paste0(dir, "data/lake.csv"))
lake_data <- data.frame(date=as.Date(glm_out$time),
swt=glm_out$Surface.Temp,
v=glm_out$Volume,
lh=glm_out$Daily.Qe,
sh=glm_out$Daily.Qh,
light=glm_out$Light,
strat=glm_out$Max.dT.dz)
#merge all possible drivers
#drivers <- merge(stream, lake_data, by="date")
#drivers <- merge(drivers, soil_meteo, by="date")
drivers <- merge(lake_data, soil_meteo, by="date")
#load GWLF: discharge and DOC
GWLF <- read.csv(paste0(dir, "data/GWLF.csv"), header = T)
GWLF <- GWLF[,2:ncol(GWLF)]
colnames(GWLF) <- c("date", "q_gwlf", "doc_gwlf")
GWLF$date <- as.Date(GWLF$date, "%m/%d/%Y")
drivers <- merge(drivers, GWLF, by="date")
#drivers <- merge(drivers, DOC, by="date")
write.csv(drivers, paste0(dir,"data/drivers.csv"), row.names = F, quote = F)
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom" #"doc"#"fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
#data$yday <- yday(data$date)
data$cyday <- cos(yday(data$date)*pi/180)
#data$syday <- sin(yday(data$date)/365)
data$random <- runif(nrow(data))
plot(data$cyday)
data$cyday <- cos(yday(data$date)*pi/180)
data$random <- runif(nrow(data))
View(data)
library(lubridate); library(dplyr)
#soil and meteo data from Open Meteo
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
soil_meteo <- read.csv(paste0(dir, "download/meteo-soil_data.csv"))
soil_meteo$date <- as.Date(soil_meteo$date)
soil_meteo_temp <- aggregate(. ~ date, data=soil_meteo, FUN = function(x) c(mean = mean(x, na.rm = TRUE)))
soil_meteo_temp$precipitation <- NULL
rain_temp <- aggregate(precipitation ~ date, data=soil_meteo, FUN = function(x) c(sum = sum(x, na.rm = TRUE)))
soil_meteo <- merge(soil_meteo_temp, rain_temp, by="date")
names(soil_meteo) <- c("date","t", "rh", "sp", "cc", "pev",
"ws","st7", "st28", "st100","st255",
"sm7", "sm28", "sm100","sm255","tp")
soil_meteo_temp <- NULL
rain_temp <- NULL
write.csv(soil_meteo, paste0(dir, "data/meteo-soil_daily_data.csv"),
row.names = F, quote = F)
#streamflow data from ATL
#stream <- read.csv(paste0(dir, "data/river.csv"), header = F)
#stream <- data.frame(date=as.Date(stream$V1, "%m/%d/%Y"), q=stream$V2)
#Load lake data (GLM output)
glm_out <- read.csv(paste0(dir, "data/lake.csv"))
lake_data <- data.frame(date=as.Date(glm_out$time),
swt=glm_out$Surface.Temp,
v=glm_out$Volume,
lh=glm_out$Daily.Qe,
sh=glm_out$Daily.Qh,
#light=glm_out$Light,
strat=glm_out$Max.dT.dz)
#merge all possible drivers
#drivers <- merge(stream, lake_data, by="date")
#drivers <- merge(drivers, soil_meteo, by="date")
drivers <- merge(lake_data, soil_meteo, by="date")
#load GWLF: discharge and DOC
GWLF <- read.csv(paste0(dir, "data/GWLF.csv"), header = T)
GWLF <- GWLF[,2:ncol(GWLF)]
colnames(GWLF) <- c("date", "q_gwlf", "doc_gwlf")
GWLF$date <- as.Date(GWLF$date, "%m/%d/%Y")
drivers <- merge(drivers, GWLF, by="date")
#load DOC
#DOC <- read.csv("sau/data/DOC_SAU_C1.csv")
#DOC <- data.frame(date=DOC$Fecha, depth=DOC$Depth, doc=DOC$Valor)
#DOC <- subset(DOC, depth==5)
#DOC <- subset(DOC, depth >= 0 & depth <= 5)
#DOC$date <- as.Date(DOC$date)
#drivers <- merge(drivers, DOC, by="date")
write.csv(drivers, paste0(dir,"data/drivers.csv"), row.names = F, quote = F)
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom" #"doc"#"fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
data$cyday <- cos(yday(data$date)*pi/180)
data$random <- runif(nrow(data))
#ML Analysis
###############################################################
#Random forest
library(randomForest)
save_rsq <- c()
save_rmse <- c()
#1. TRAINING WITH ALL DATA AND TESTING WITH SAMPLES OOB
#in theory this is wrong because the trianing data is autocorrelated
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 1000)
#testing: check with data not used in training but still autocorrelated (Out-of-Bag data)
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, data[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((data[tvar][,1] - predRF)^2)), 2); rmseOOB
importance_random <- importance(RFfit); importance_random
importance_random/sum(importance_random)*100
plot(data$cyday, data$fdom)
#fdom data correction: temperature effect
#table from manufacturer (YSI)
temp_effect <- c(30, 28, 26, 24, 22, 20, 18, 16, 14, 12, 10, 8)
qsu_effect <- c(289.2, 291.9, 294.6, 297.3, 300, 302.7, 305.4, 308.1, 310.8, 313.8, 316.5, 319.2)
#fdom data correction: temperature effect
#table from manufacturer (YSI)
temp_effect <- c(30, 28, 26, 24, 22, 20, 18, 16, 14, 12, 10, 8)
qsu_effect <- c(289.2, 291.9, 294.6, 297.3, 300, 302.7, 305.4, 308.1, 310.8, 313.8, 316.5, 319.2)
plot(temp_effect, qsu_effect)
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom" #"doc"#"fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
data$cyday <- cos(yday(data$date)*pi/180)
data$random <- runif(nrow(data))
plot(data$t, data$fdom)
plot(data$fdom, data$t)
#fdom data correction: temperature effect
#table from manufacturer (YSI)
temp_effect <- c(30, 28, 26, 24, 22, 20, 18, 16, 14, 12, 10, 8)
qsu_effect <- c(289.2, 291.9, 294.6, 297.3, 300, 302.7, 305.4, 308.1, 310.8, 313.8, 316.5, 319.2)
library(lubridate)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom" #"doc"#"fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
data$cyday <- cos(yday(data$date)*pi/180)
data$random <- runif(nrow(data))
lm(data$fdom ~ data$t)
regression <- lm(data$fdom ~ data$t)
regression$coefficients
reticulate::repl_python()
