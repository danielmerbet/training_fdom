rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
library(lubridate)
set.seed(123)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
number_test <- 100
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
data$cyday <- cos(yday(data$date)*pi/180)
data$random <- runif(nrow(data))
nse <- function(sim, obs) {
numerator <- sum((obs - sim)^2)
denominator <- sum((obs - mean(obs))^2)
nse <- 1 - (numerator / denominator)
return(nse)
}
#ML Analysis
###############################################################
#Random forest
library(randomForest)
save_drivers <- c();save_importance <- c(); save_perc <- c()
save_rsq <- c(); save_rmse <- c(); save_nse <- c()
train_perc <- 0.85 #percentage for training
training_number <- round(dim(data)[1]*train_perc)
total_front <- dim(data)[1]-training_number
for (i in 0:number_test){
if (i < total_front){
m <- (1+i):(1+i+training_number)
}else{
break
}
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2)
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
save_nse <- c(save_nse, nse_test)
importance_random <- importance(RFfit); importance_random
importance_perc <- importance_random/sum(importance_random)*100
save_drivers <- cbind(save_drivers, rownames(importance_perc))
save_importance <- cbind(save_importance, importance_random)
save_perc <- cbind(save_perc, importance_perc)
}
max(save_nse)
save_nse==max(save_nse)
which(save_nse==max(save_nse))
which(save_rmse==min(save_rmse))
which(save_rsq==max(save_rsq))
which(save_nse==max(save_nse))
max(save_nse)
save_rmse[65]
save_rmse[68]
save_rsq[68]
save_rsq[65]
which(save_nse==max(save_nse))
which(save_rmse==min(save_rmse))
which(save_rsq==max(save_rsq))
library(lubridate)
set.seed(123)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom" #"doc"#"fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
data$cyday <- cos(yday(data$date)*pi/180)
data$random <- runif(nrow(data))
library(randomForest)
save_rsq <- c()
save_rmse <- c()
#1. TRAINING WITH ALL DATA AND TESTING WITH SAMPLES OOB
#in theory this is wrong because the trianing data is autocorrelated
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 1000)
#testing: check with data not used in training but still autocorrelated (Out-of-Bag data)
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, data[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((data[tvar][,1] - predRF)^2)), 2); rmseOOB
importance_random <- importance(RFfit); importance_random
importance_perc <- importance_random/sum(importance_random)*100
importance_perc
#stats are quite good, it seems promising
plot(data$date, data[tvar][,1], xlab="Dates", ylab="fDOM (QSU)")
points(data$date, predRF, col="darkgrey")
plot(data[tvar][,1],predRF, xlab="Obs", ylab="Sim") #ylim=c(5,52),xlim=c(5,52)
abline(0,1, col="red")
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.85 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
best <- 65
training_number <- round(dim(data)[1]*train_perc)
m <- (1+best):(1+best+training_number)
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
best
(1+best)
(1+best+training_number)
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.85 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
best <- 65
training_number <- round(dim(data)[1]*train_perc)
m <- (1+best):(1+best+training_number)
m
traindata <- data[m,]
-m
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
View(data)
View(testdata)
nse <- function(sim, obs) {
numerator <- sum((obs - sim)^2)
denominator <- sum((obs - mean(obs))^2)
nse <- 1 - (numerator / denominator)
return(nse)
}
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.85 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
best <- 65
training_number <- round(dim(data)[1]*train_perc)
m <- (1+best):(1+best+training_number)
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2); nse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.85 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
best <- 101
training_number <- round(dim(data)[1]*train_perc)
m <- (1+best):(1+best+training_number)
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2); nse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.15 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
traindata <- data[-m,]
testdata <- data[m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2); nse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
View(drivers)
View(data)
data[c(25,26)]
data[-c(25,26)]
data <- data[-c(25,26)]
nse <- function(sim, obs) {
numerator <- sum((obs - sim)^2)
denominator <- sum((obs - mean(obs))^2)
nse <- 1 - (numerator / denominator)
return(nse)
}
library(randomForest)
save_rsq <- c()
save_rmse <- c()
#1. TRAINING WITH ALL DATA AND TESTING WITH SAMPLES OOB
#in theory this is wrong because the trianing data is autocorrelated
#train RF
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = data, ntree = 1000)
#testing: check with data not used in training but still autocorrelated (Out-of-Bag data)
predRF<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB = round((cor(predRF, data[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((data[tvar][,1] - predRF)^2)), 2); rmseOOB
importance_random <- importance(RFfit); importance_random
importance_perc <- importance_random/sum(importance_random)*100
importance_perc
#stats are quite good, it seems promising
plot(data$date, data[tvar][,1], xlab="Dates", ylab="fDOM (QSU)")
points(data$date, predRF, col="darkgrey")
plot(data[tvar][,1],predRF, xlab="Obs", ylab="Sim") #ylim=c(5,52),xlim=c(5,52)
abline(0,1, col="red")
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.15 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
best <- 101
training_number <- round(dim(data)[1]*train_perc)
m <- (1+best):(1+best+training_number)
traindata <- data[-m,]
testdata <- data[m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2); nse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
dim(data)[1]-training_number
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.15 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
best <- 101
training_number <- round(dim(data)[1]*train_perc)
m <- (1+best):(1+best+training_number)
traindata <- data[-m,]
testdata <- data[m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2); nse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.15 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
best <- 101
training_number <- round(dim(data)[1]*train_perc)
traindata <- data[-m,]
testdata <- data[m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2); nse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
#2. TRAINING AND TESTING WITH NON AUTOCORRELATED SAMPLES
#OOB samples are independent,
#but, RF assumes observations are not autocorrelated
#For time series, this assumption might not hold
#Let's train and test with samples taken from different time ranges
train_perc <- 0.85 #percentage for training
m <- 1:(dim(data)[1]*train_perc)
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#check resulting stats with OOB data
predRF_OOB<- predict(RFfit) # without data, give the prediction with OOB samples
rsqOOB <- round((cor(predRF_OOB, traindata[tvar]))^2,2) ; rsqOOB
rmseOOB <- round(sqrt(mean((traindata[tvar][,1] - predRF_OOB)^2)), 2); rmseOOB
maeOOB <- mean(abs(traindata[tvar][,1] - predRF_OOB));maeOOB
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2); nse_test
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
maeOOB <- mean(abs(testdata[tvar][,1] - predRF));maeOOB
importance_random <- importance(RFfit); importance_random
#now the stats are NOT as good, but they are REAL (not affected by autocorrelation)
plot(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", type="l")
points(data$date, data[tvar][,1], ylab="fDOM (QSU)", xlab = "", xaxt='n')
points(traindata$date, predRF_OOB, col="blue", pch = 19, cex=0.5)
#plot(testdata$date, testdata[tvar][,1], ylim=c(5,52), xlab="Dates", ylab="fDOM (QSU)")
points(testdata$date, predRF, col="red", pch = 19, cex=0.5)
library(lubridate)
set.seed(123)
case_study <- "sau"
dir <- paste0("~/Documents/intoDBP/training_fdom/",case_study, "/")
number_test <- 150
#load drivers (meteorology, soil,  streamflow and all possible variables)
drivers <- read.csv(paste0(dir, "data/drivers.csv"))
drivers$date <- as.Date(drivers$date)
#load target variables
tvar <- "fdom"
target <- read.csv(paste0(dir ,"data/",tvar,".csv"))
target$date <- as.Date(target$date)
#merge all and add julian day and dummy
data <- merge(drivers, target, by="date")
data$cyday <- cos(yday(data$date)*pi/180)
data$random <- runif(nrow(data))
nse <- function(sim, obs) {
numerator <- sum((obs - sim)^2)
denominator <- sum((obs - mean(obs))^2)
nse <- 1 - (numerator / denominator)
return(nse)
}
#ML Analysis
###############################################################
#Random forest
library(randomForest)
save_drivers <- c();save_importance <- c(); save_perc <- c()
save_rsq <- c(); save_rmse <- c(); save_nse <- c()
train_perc <- 0.85 #percentage for training
training_number <- round(dim(data)[1]*train_perc)
total_front <- dim(data)[1]-training_number
for (i in 0:number_test){
if (i < total_front){
m <- (1+i):(1+i+training_number)
}else{
break
}
traindata <- data[m,]
testdata <- data[-m,]
#start training
formula <- as.formula(paste(tvar, "~ . - date"))
RFfit <- randomForest(formula, data = traindata, ntree = 1000)
#testing: check with data not used in training
predRF<- predict(RFfit, testdata) # without data, give the prediction with OOB samples
rsq_test <- round((cor(predRF, testdata[tvar]))^2,2) ; rsq_test
rmse_test <- round(sqrt(mean((testdata[tvar][,1] - predRF)^2)), 2); rmse_test
nse_test <- round(nse(testdata[tvar][,1], predRF),2)
save_rsq <- c(save_rsq, rsq_test)
save_rmse <- c(save_rmse, rmse_test)
save_nse <- c(save_nse, nse_test)
importance_random <- importance(RFfit); importance_random
importance_perc <- importance_random/sum(importance_random)*100
save_drivers <- cbind(save_drivers, rownames(importance_perc))
save_importance <- cbind(save_importance, importance_random)
save_perc <- cbind(save_perc, importance_perc)
}
